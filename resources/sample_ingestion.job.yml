resources:
  jobs:  # https://docs.databricks.com/aws/en/dev-tools/bundles/resources#job
    sample_ingestion:
      name: Sample Ingestion Job
      max_concurrent_runs: 1
      timeout_seconds: 0  # No timeout

      email_notifications:
        on_failure: ${var.failure_notification_emails}

      notification_settings:
        no_alert_for_skipped_runs: true
        no_alert_for_canceled_runs: true

      # OPTION 1: Serverless compute
      environments:
        - environment_key: default
          # https://docs.databricks.com/api/workspace/jobs/create#environments-spec
          spec:
            environment_version: "2"

      # OPTION 2: Custom cluster
      # Uncomment the section below and comment out the 'environments' section above
      # This provides better control over cluster specs, libraries, and configuration
      # Template source: templates/cluster_configs.yml (autoscale_cluster)
      # job_clusters:
      #   - job_cluster_key: job_cluster
      #     new_cluster:
      #       spark_version: 17.3.x-scala2.13
      #       node_type_id: Standard_DS3_v2  # AWS: i3.xlarge; Azure: Standard_DS3_v2; GCP: n1-highmem-4
      #       autoscale:
      #         min_workers: ${var.job_cluster_min_workers}
      #         max_workers: ${var.job_cluster_max_workers}
      #       autotermination_minutes: 10
      #       custom_tags:
      #         bundle: ${bundle.name}
      #         environment: ${bundle.target}
      #       spark_conf:
      #         "spark.databricks.delta.autoCompact.enabled": "auto"
      #         "spark.databricks.delta.optimizeWrite.enabled": "true"
      #       spark_env_vars:
      #         ENVIRONMENT: ${bundle.target}

      tasks:  # https://docs.databricks.com/aws/en/dev-tools/bundles/job-task-types#other-task-settings
        - task_key: ingest_to_raw
          description: Ingest sample data to raw layer

          # For serverless: use environment_key
          environment_key: default

          # For custom cluster: uncomment and use job_cluster_key instead
          # job_cluster_key: job_cluster

          max_retries: ${var.max_retries}
          min_retry_interval_millis: ${var.retry_interval_millis}

          spark_python_task:
            python_file: ${workspace.file_path}/src/jobs/ingest_to_raw.py
            parameters:
              - --catalog_name
              - ${var.catalog_name}
              - --environment
              - ${bundle.target}
              - --user_name
              - ${workspace.current_user.short_name}

        # Second task: Transform bronze data to silver layer
        - task_key: transform_to_silver
          description: Transform raw data to silver layer
          depends_on:
            - task_key: ingest_to_raw

          # For serverless: use environment_key
          environment_key: default

          # For custom cluster: uncomment and use job_cluster_key instead
          # job_cluster_key: job_cluster

          max_retries: ${var.max_retries}
          min_retry_interval_millis: ${var.retry_interval_millis}

          spark_python_task:
            python_file: ${workspace.file_path}/src/jobs/transform_to_silver.py
            parameters:
              - --catalog_name
              - ${var.catalog_name}
              - --environment
              - ${bundle.target}
              - --user_name
              - ${workspace.current_user.short_name}