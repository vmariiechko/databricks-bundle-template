resources:
  pipelines:  # https://docs.databricks.com/aws/en/dev-tools/bundles/resources#pipeline
    sample_pipeline:
      name: Sample ETL Pipeline
      edition: ADVANCED
      channel: CURRENT

      catalog: ${var.catalog_name}
      # Shared default; bundle targets override schema when needed
      schema: ${var.default_schema_name}
      photon: ${var.photon_enabled}
      continuous: ${var.continuous_mode}

      # Specify the FQN for the pipeline event logs table
      event_log:
        catalog: ${var.catalog_name}
        schema: ${var.default_schema_name}
        name: pipeline_event_log

      # DLT configuration passed to notebooks
      configuration:
        "spark.databricks.delta.autoCompact.enabled": "true"
        "spark.databricks.delta.optimizeWrite.enabled": "true"
        "catalogName": ${var.catalog_name}
        "pipelineEnvironment": ${bundle.target}
        "userName": ${workspace.current_user.short_name}

      # OPTION 1: Serverless compute
      serverless: true

      # OPTION 2: Custom cluster for DLT
      # Uncomment the section below and set serverless: false
      # clusters:
      #   - <<: *pipeline_cluster_autoscale  # Reference to cluster_configs.yml

      # Source code - referencing *.py files in Databricks Notebook format
      libraries:
        - notebook:
            path: ${workspace.file_path}/src/pipelines/bronze
        - notebook:
            path: ${workspace.file_path}/src/pipelines/silver

