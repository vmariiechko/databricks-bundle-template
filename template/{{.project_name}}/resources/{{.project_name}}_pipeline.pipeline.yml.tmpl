resources:
  pipelines:  # https://docs.databricks.com/aws/en/dev-tools/bundles/resources#pipeline
    {{.project_name}}_pipeline:
      name: {{.project_name}} ETL Pipeline
      edition: ADVANCED
      channel: CURRENT

      catalog: ${var.catalog_name}
      # Schema includes prefix for per-user isolation in development
      schema: ${var.schema_prefix}${var.default_schema_name}
      photon: ${var.photon_enabled}
      continuous: ${var.continuous_mode}

      # Specify the FQN for the pipeline event logs table
      event_log:
        catalog: ${var.catalog_name}
        schema: ${var.schema_prefix}${var.default_schema_name}
        name: pipeline_event_log

      # LDP configuration passed to notebooks
      configuration:
        "spark.databricks.delta.autoCompact.enabled": "true"
        "spark.databricks.delta.optimizeWrite.enabled": "true"
        "catalogName": ${var.catalog_name}
        "schemaPrefix": ${var.schema_prefix}
        "pipelineEnvironment": ${bundle.target}
        "userName": ${workspace.current_user.short_name}

{{- if or (eq .compute_type "serverless") (eq .compute_type "both") }}
      # Serverless compute
      serverless: true
{{- end }}

{{- if eq .compute_type "classic" }}
      # Classic cluster compute for LDP
      serverless: false
      clusters:
        - label: default
          node_type_id: {{template "node_type_id" .}}
          autoscale:
            mode: ENHANCED
            min_workers: ${var.pipeline_min_workers}
            max_workers: ${var.pipeline_max_workers}
          custom_tags:
            bundle: ${bundle.name}
            environment: ${bundle.target}
{{- end }}

{{- if eq .compute_type "both" }}

      # Classic cluster compute for LDP (alternative)
      # To use classic compute: set `serverless: false` and uncomment `clusters`
      # clusters:
      #   - label: default
      #     node_type_id: {{template "node_type_id" .}}
      #     autoscale:
      #       mode: ENHANCED
      #       min_workers: ${var.pipeline_min_workers}
      #       max_workers: ${var.pipeline_max_workers}
      #     custom_tags:
      #       bundle: ${bundle.name}
      #       environment: ${bundle.target}
{{- end }}

      # Python dependencies for the pipeline
      # Add any PyPI packages your pipeline code needs
      # Docs: https://docs.databricks.com/aws/en/dev-tools/bundles/resources#pipelineenvironment
      # environment:
      #   dependencies:
      #     - requests==2.32.3
      #     - simplejson==3.19.2

      # Source code - referencing *.py files in Databricks Notebook format
      libraries:
        - notebook:
            path: ../src/pipelines/bronze.py
        - notebook:
            path: ../src/pipelines/silver.py
