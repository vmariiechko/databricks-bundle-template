resources:
  jobs:  # https://docs.databricks.com/aws/en/dev-tools/bundles/resources#job
    {{.project_name}}_ingestion:
      name: {{.project_name}} Ingestion Job
      max_concurrent_runs: 1
      timeout_seconds: 0  # No timeout

      email_notifications:
        on_failure: ${var.failure_notification_emails}

      notification_settings:
        no_alert_for_skipped_runs: true
        no_alert_for_canceled_runs: true

      # Health rules for job monitoring
      # Docs: https://docs.databricks.com/aws/en/dev-tools/bundles/resources#job-health-rules
      health:
        rules:
          - metric: RUN_DURATION_SECONDS
            op: GREATER_THAN
            value: 3600  # Alert if job takes > 1 hour

{{- if or (eq .compute_type "serverless") (eq .compute_type "both") }}

      # Serverless compute
      environments:
        - environment_key: default
          # https://docs.databricks.com/api/workspace/jobs/create#environments-spec
          spec:
            environment_version: "2"
            # Add your packages here
            dependencies:
              - requests==2.32.3
              - databricks-sql-connector==4.0.0
{{- end }}

{{- if or (eq .compute_type "classic") (eq .compute_type "both") }}

      # Classic cluster compute
{{- if eq .compute_type "both" }}
      # NOTE: To use classic compute, uncomment job_clusters and update task references
      # job_clusters:
{{- else }}
      job_clusters:
{{- end }}
{{- if eq .compute_type "both" }}
      #   - job_cluster_key: job_cluster
      #     new_cluster:
      #       spark_version: {{template "spark_version" .}}
      #       node_type_id: {{template "node_type_id" .}}
      #       autoscale:
      #         min_workers: ${var.job_cluster_min_workers}
      #         max_workers: ${var.job_cluster_max_workers}
      #       autotermination_minutes: 10
      #       custom_tags:
      #         bundle: ${bundle.name}
      #         environment: ${bundle.target}
      #       spark_conf:
      #         "spark.databricks.delta.autoCompact.enabled": "auto"
      #         "spark.databricks.delta.optimizeWrite.enabled": "true"
      #       spark_env_vars:
      #         ENVIRONMENT: ${bundle.target}
{{- else }}
        - job_cluster_key: job_cluster
          new_cluster:
            spark_version: {{template "spark_version" .}}
            node_type_id: {{template "node_type_id" .}}
            autoscale:
              min_workers: ${var.job_cluster_min_workers}
              max_workers: ${var.job_cluster_max_workers}
            autotermination_minutes: 10
            custom_tags:
              bundle: ${bundle.name}
              environment: ${bundle.target}
            spark_conf:
              "spark.databricks.delta.autoCompact.enabled": "auto"
              "spark.databricks.delta.optimizeWrite.enabled": "true"
            spark_env_vars:
              ENVIRONMENT: ${bundle.target}
{{- end }}
{{- end }}

      tasks:  # https://docs.databricks.com/aws/en/dev-tools/bundles/job-task-types#other-task-settings
        - task_key: ingest_to_raw
          description: Ingest sample data to raw layer

{{- if or (eq .compute_type "serverless") (eq .compute_type "both") }}
          # Serverless compute
          environment_key: default
{{- end }}

{{- if eq .compute_type "classic" }}
          # Classic cluster compute
          job_cluster_key: job_cluster
          libraries:
            - pypi:
                package: requests==2.32.3
            - pypi:
                package: databricks-sql-connector==4.0.0
{{- end }}

{{- if eq .compute_type "both" }}
          # For classic compute: uncomment job_cluster_key and libraries, comment out environment_key
          # job_cluster_key: job_cluster
          # libraries:
          #   - pypi:
          #       package: requests==2.32.3
          #   - pypi:
          #       package: databricks-sql-connector==4.0.0
{{- end }}

          max_retries: ${var.max_retries}
          min_retry_interval_millis: ${var.retry_interval_millis}

          spark_python_task:
            python_file: ../src/jobs/ingest_to_raw.py
            parameters:
              - --catalog_name
              - ${var.catalog_name}
              - --environment
              - ${bundle.target}
              - --user_name
              - ${workspace.current_user.short_name}

        # Second task: Transform bronze data to silver layer
        - task_key: transform_to_silver
          description: Transform raw data to silver layer
          depends_on:
            - task_key: ingest_to_raw

{{- if or (eq .compute_type "serverless") (eq .compute_type "both") }}
          # Serverless compute
          environment_key: default
{{- end }}

{{- if eq .compute_type "classic" }}
          # Classic cluster compute
          job_cluster_key: job_cluster
          libraries:
            - pypi:
                package: requests==2.32.3
{{- end }}

{{- if eq .compute_type "both" }}
          # For classic compute: uncomment job_cluster_key and libraries, comment out environment_key
          # job_cluster_key: job_cluster
          # libraries:
          #   - pypi:
          #       package: requests==2.32.3
{{- end }}

          max_retries: ${var.max_retries}
          min_retry_interval_millis: ${var.retry_interval_millis}

          spark_python_task:
            python_file: ../src/jobs/transform_to_silver.py
            parameters:
              - --catalog_name
              - ${var.catalog_name}
              - --environment
              - ${bundle.target}
              - --user_name
              - ${workspace.current_user.short_name}
