# Cluster configuration definitions using YAML anchors
# These are reusable cluster configs that can be referenced in resource files
# Use these when you want custom clusters instead of serverless compute

# Small cluster for development/testing
small_cluster: &small_cluster
  num_workers: 1
  spark_version: 17.3.x-scala2.13
  node_type_id: Standard_D3_v2  #AWS: i3.xlarge; Azure: Standard_D3_v2; GCP: n1-highmem-4
  autotermination_minutes: 10
  custom_tags:
    bundle: ${bundle.name}
    environment: ${bundle.target}
  spark_conf:
    "spark.databricks.delta.autoCompact.enabled": "auto"
  spark_env_vars:
    ENVIRONMENT: ${bundle.target}

# Medium cluster for regular workloads
medium_cluster: &medium_cluster
  num_workers: 2
  spark_version: 17.3.x-scala2.13
  node_type_id: Standard_D3_v2
  autotermination_minutes: 10
  custom_tags:
    bundle: ${bundle.name}
    environment: ${bundle.target}
  spark_conf:
    "spark.databricks.delta.autoCompact.enabled": "auto"
    "spark.databricks.delta.optimizeWrite.enabled": "true"
  spark_env_vars:
    ENVIRONMENT: ${bundle.target}

# Autoscaling cluster for variable workloads
autoscale_cluster: &autoscale_cluster
  spark_version: 17.3.x-scala2.13
  node_type_id: Standard_D3_v2
  autoscale:
    min_workers: ${var.job_cluster_min_workers}
    max_workers: ${var.job_cluster_max_workers}
  autotermination_minutes: 10
  custom_tags:
    bundle: ${bundle.name}
    environment: ${bundle.target}
  spark_conf:
    "spark.databricks.delta.autoCompact.enabled": "auto"
    "spark.databricks.delta.optimizeWrite.enabled": "true"
  spark_env_vars:
    ENVIRONMENT: ${bundle.target}

# Photon-enabled cluster for high-performance workloads
photon_cluster: &photon_cluster
  spark_version: 17.3.x-scala2.13
  node_type_id: Standard_D3_v2
  runtime_engine: PHOTON
  autoscale:
    min_workers: ${var.job_cluster_min_workers}
    max_workers: ${var.job_cluster_max_workers}
  autotermination_minutes: 10
  custom_tags:
    bundle: ${bundle.name}
    environment: ${bundle.target}
  spark_conf:
    "spark.databricks.delta.autoCompact.enabled": "auto"
    "spark.databricks.delta.optimizeWrite.enabled": "true"
  spark_env_vars:
    ENVIRONMENT: ${bundle.target}

# Pipeline cluster configuration for DLT
# Note: DLT pipelines have slightly different cluster syntax
pipeline_cluster_small: &pipeline_cluster_small
  label: default
  num_workers: 1
  node_type_id: Standard_D3_v2
  custom_tags:
    bundle: ${bundle.name}
    environment: ${bundle.target}

pipeline_cluster_autoscale: &pipeline_cluster_autoscale
  label: default
  node_type_id: Standard_D3_v2
  autoscale:
    mode: ENHANCED
    min_workers: ${var.pipeline_min_workers}
    max_workers: ${var.pipeline_max_workers}
  custom_tags:
    bundle: ${bundle.name}
    environment: ${bundle.target}
