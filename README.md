# TODO MY NOTES:
- [x] TEST dev/stage/prod permissions deployment
- [x] Configuring schema creation using DABs with permissions
- [x] Fix to make sure create UC per user, not per schema
- [x] Remove `permissions.yml` from repo
- [x] Review the final setup of permissions - generated files etc.
- [ ] Fix `cluster_configs.yml` as I can't use anchoring feature in different files -> think to move to core bundle and override clusters

05.11: switched a bit to Job tasks configuration - (FINISHED - TEST) wanna generate `depends_on` task.
 - [ ] Explore task options; consider `retry_on_timeout`: https://docs.databricks.com/aws/en/dev-tools/bundles/job-task-types#other-task-settings
 - [ ]

03.11: STOPPED configuring clusters in `cluster_configs.yml` -> all looks great; also analyzed sample manually created LDP in `example_manual_pipeline.yaml`; continue:
  - [ ] Look into `permissions:` key - search it in bundle-samples

# Useful commands

```
databricks bundle deploy -t prod --var="dev_service_principal=6b3a2d15-6f48-4d19-a56b-194a5dcdc234,stage_service_principal=c6c5d56a-2a4b-4a89-b44d-acb7d3f50631,prod_service_principal=139b87a5-53c4-4884-90eb-1dd37f8a2e4e" --force
```

# Hand-picked Docs
* Job settings: https://docs.databricks.com/api/azure/workspace/jobs/create


# MY PHILOSOPHY AS A CREATOR / HIGH-LEVEL PLAN FOR THIS REPOSITORY

The idea is right now to build a single comprehensive DABs setup for single use-case, including:
- user, dev, stage, prod environments per UC
- permissions
- all other comprehensive stuff

Then, at some point, we will:
1. Turn this use-case into as reusable template that can be configured and used as a "Custom Template" via databricks bundles CLI.
2. Note: at some point I was considering to add other "use cases" for different environments management (UC-based separation, schema-based separation etc.). Each use case could be in separate folder with its template. But I decided that it's better to not complicate/overengineer setup and keep single suggested use-case.
3. Add plugins layer - the complexity of template might vary. I'm looking at it as a building blocks, so that for example, some teams don't want to have such complex permissions setup. During the template configuration, when they are prompted in the CLI, we can ask them whether they want a block for overall permissions setup or completely omit it. As a result, I will populate and plug in those blocks into the final template.

## TODOs from philosophy

- [ ] Include prerequisites to that Unity Catalogs have to created (specified in `catalog_name` variable); service principals created
- [ ] I don't really like the current folder structure that it contains variables and cluster_configs in separate files and it's all in the root repository folder. I want to move every related Databricks bundles stuff to one folder, maybe have some nested structure there, and have only @databricks.yml file at repository root.

- [ ] Turn this repo into template.
- [ ] Add CI/CD configuration.
- [ ] When Creating template:
  - [ ] Allow user to select cluster - serverless / custom - prefer to use custom; maybe even just leave serverless commented out or completely remove.



----- BELOW CONTENT GENERATED BY LLM -----

# Databricks Multi-Environment Deployment Template

A production-ready template for deploying Databricks Asset Bundles across multiple environments (user, dev, stage, prod) with environment-specific configurations.

**Focus:** This template demonstrates deployment patterns and environment management, not specific business logic. The sample code is intentionally minimal‚Äîreplace it with your actual workloads.

## üéØ What This Template Provides

- **Multi-environment strategy**: Isolated deployments for user, dev, stage, and prod
- **Environment-specific overrides**: Different compute, schedules, and schemas per environment
- **Serverless-first**: Ready for Databricks Free Edition and production
- **Modular structure**: Clean separation between resources and code
- **CI/CD ready**: Git-based deployment workflows

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ databricks.yml              # Bundle configuration with 4 environments
‚îú‚îÄ‚îÄ variables.yml               # Shared variables
‚îú‚îÄ‚îÄ resources/
‚îÇ   ‚îú‚îÄ‚îÄ sample_ingestion.job.yml         # Example: Ingestion job
‚îÇ   ‚îú‚îÄ‚îÄ sample_pipeline.pipeline.yml     # Example: DLT pipeline
‚îÇ   ‚îî‚îÄ‚îÄ sample_pipeline_trigger.job.yml  # Example: Pipeline trigger
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ jobs/
    ‚îÇ   ‚îî‚îÄ‚îÄ ingest_to_raw.py              # Sample job code (replace with yours)
‚îî‚îÄ‚îÄ pipelines/
    ‚îú‚îÄ‚îÄ bronze.py           # Sample DLT bronze layer
    ‚îî‚îÄ‚îÄ silver.py           # Sample DLT silver layer
```

## üöÄ Quick Start

### 1. Prerequisites

- Databricks workspace (Free Edition or paid)
- Databricks CLI installed: `pip install databricks-cli`
- Unity Catalog with a catalog (default: `demo-catalog`)

### 2. Deploy to Your Workspace

```bash
# Validate the bundle
databricks bundle validate -t user

# Deploy to your user environment (isolated, development mode)
databricks bundle deploy -t user

# Run the sample job
databricks bundle run sample_ingestion -t user

# Trigger the sample pipeline
databricks bundle run sample_pipeline_trigger -t user
```

### 3. Verify Deployment

Check your Databricks workspace:
- **Jobs**: Look for `[user <yourname>] Sample Ingestion Job`
- **Pipelines**: Look for `[user <yourname>] Sample ETL Pipeline`

## üåç Environment Strategy

### Four Deployment Targets

| Target | Purpose | Mode | Naming | Schedule | Compute |
|--------|---------|------|--------|----------|---------|
| **user** | Local development | development | `[user <name>]` prefix | Paused | Serverless |
| **dev** | Shared development | production | `[dev]` prefix | Active | Configurable |
| **stage** | Pre-production | production | `[stage]` prefix | Active | Production-like |
| **prod** | Production | production | No prefix | Active | Production |

### Key Differences Per Environment

```yaml
# User: Fast iteration, isolated
user:
  mode: development
  presets:
    name_prefix: "[user ${workspace.current_user.short_name}] "
    trigger_pause_status: PAUSED
    pipelines_development: true

# Dev: Shared team environment
dev:
  mode: production
  git:
    branch: dev
  # Resources optimized for cost

# Stage: Production mirror
stage:
  mode: production
  git:
    branch: stage
  run_as:
    service_principal_name: "<spn-id>"
  # Full production configuration

# Prod: Production
prod:
  mode: production
  git:
    branch: main
  run_as:
    service_principal_name: "<spn-id>"
  # Maximum reliability and performance
```

## üîí Permissions and Access Control

This template includes comprehensive permissions configuration for managing secure, role-based access to your Databricks resources across environments.

### üöÄ Quick Start

**Option 1: Just Testing** (No Setup Required)
```bash
databricks bundle deploy -t user  # Automatic full access in development mode
```

**Option 2: Team Collaboration** (5-Minute Setup)
1. Create required groups in Databricks workspace (see [docs/SETUP_GROUPS.md](./docs/SETUP_GROUPS.md))
2. Deploy: `databricks bundle deploy -t dev`

**Option 3: Production Ready** (Full Setup)
1. Create service principals
2. Configure in `variables.yml`
3. Enable in `databricks.yml`

### üìñ Documentation

| Document | Purpose |
|----------|---------|
| **[docs/SETUP_GROUPS.md](./docs/SETUP_GROUPS.md)** | How to create required groups before deployment |
| **[docs/PERMISSIONS_SETUP.md](./docs/PERMISSIONS_SETUP.md)** | Complete setup guide with examples |
| **[docs/PERMISSIONS_REFERENCE.md](./docs/PERMISSIONS_REFERENCE.md)** | Comprehensive reference and advanced patterns |

### üéØ Key Features

- **Two permission systems**: Resource permissions (jobs/pipelines) + Unity Catalog grants (schemas)
- **Zero config in dev**: Works immediately in `user` target (development mode)
- **Environment-specific**: Different access levels for dev/stage/prod
- **Service principal ready**: Automated, auditable deployments
- **Group-based**: Scalable access management

## üîß Customization

### Replace Sample Code

The `src/` directory contains minimal example code that demonstrates the structure works. **Replace it with your actual code:**

1. **Jobs** (`src/jobs/run.py`): Replace with your ingestion logic
2. **Pipelines** (`src/pipelines/*.py`): Replace with your transformations

### Add Resources

Create new resource files in `resources/`:

```yaml
# resources/my_new_job.job.yml
resources:
  jobs:
    my_new_job:
      name: "My New Job"
      tasks:
        - task_key: main
          spark_python_task:
            python_file: "${workspace.file_path}/src/my_script.py"
```

### Environment-Specific Overrides

Override any resource setting per environment in `databricks.yml`:

```yaml
targets:
  user:
    resources:
      pipelines:
        sample_pipeline:
          schema: "${workspace.current_user.short_name}_schema"
          continuous: false

  prod:
    resources:
      pipelines:
        sample_pipeline:
          schema: "prod_schema"
          continuous: true
          clusters:
            - label: default
              autoscale:
                min_workers: 5
                max_workers: 20
```

## üìä Sample Code Overview

The included code is intentionally simple to demonstrate the pattern:

### Ingestion Job (`src/jobs/run.py`)
- Reads from `samples.nyctaxi.trips` (built-in Databricks sample data)
- Prints output to verify the job runs
- **Replace with**: Your actual data ingestion logic

### Bronze Layer (`src/pipelines/bronze.py`)
- Creates a `sample_bronze` table
- Adds ingestion timestamp
- **Replace with**: Your bronze layer transformations

### Silver Layer (`src/pipelines/silver.py`)
- Aggregates bronze data by pickup zone
- **Replace with**: Your silver layer business logic

## üîÑ CI/CD Workflow

### Recommended Git Workflow

```
feature/my-work ‚Üí dev branch ‚Üí stage branch ‚Üí main branch
     ‚Üì               ‚Üì             ‚Üì             ‚Üì
   (local)      dev target    stage target   prod target
```

### Example GitHub Actions

```yaml
name: Deploy to Dev
on:
  push:
    branches: [dev]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy bundle
        run: |
          pip install databricks-cli
          databricks bundle deploy -t dev
        env:
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
```

## üìã Configuration Variables

Edit `variables.yml` for environment-agnostic settings:

```yaml
variables:
  catalog:
    description: UC catalog to deploy into
    default: "`demo-catalog`"

  failure_notification_emails:
    description: Alert recipients
    default: []

  pipeline_min_workers:
    description: Default min workers for pipeline
    default: 1

  photon_enabled:
    description: Enable Photon acceleration
    default: true
```

## üí° Free Edition Notes

The template works on Databricks Free Edition:

- ‚úÖ Uses serverless compute by default
- ‚úÖ Jobs and pipelines supported
- ‚úÖ No custom clusters required
- ‚ö†Ô∏è Limited to 1 active pipeline per type

To use custom clusters (paid workspaces), uncomment the cluster configurations in `databricks.yml` and resource files.

## üß™ Testing

```bash
# Validate configuration
databricks bundle validate -t user

# Deploy and test
databricks bundle deploy -t user
databricks bundle run sample_ingestion -t user

# Clean up
databricks bundle destroy -t user
```

## üîç What to Modify for Your Use Case

1. **Bundle name** (`databricks.yml`): Change `realworld_example` to your project name
2. **Sample code** (`src/`): Replace with your actual business logic
3. **Resource files** (`resources/`): Add/modify jobs and pipelines
4. **Variables** (`variables.yml`): Add your configuration variables
5. **Workspace hosts** (`databricks.yml`): Update placeholder URLs
6. **Service principals** (`databricks.yml`): Update for stage/prod (optional)
7. **Permissions** (`databricks.yml`): Configure access control for your organization (optional)

## üìö Additional Resources

- [Databricks Asset Bundles Documentation](https://docs.databricks.com/dev-tools/bundles/)
- [Deployment Modes](https://docs.databricks.com/dev-tools/bundles/deployment-modes.html)
- [Delta Live Tables](https://docs.databricks.com/delta-live-tables/)
- [Serverless Compute](https://docs.databricks.com/serverless-compute/)

## ü§ù Contributing

This is a template repository. Fork it, customize it, and make it your own!

## üìù License

This template is provided as-is for use in your Databricks projects.
