# TODO MY NOTES:
10.11: reviewing permissions setup
- [ ] TEST dev/stage/prod permissions deployment
- [ ] stopped reviewing "Resource-Specific Permission Templates"
- [ ] Fix `cluster_configs.yml` as I can't use anchoring feature in different files -> think to move to core bundle and override clusters

05.11: switched a bit to Job tasks configuration - (FINISHED - TEST) wanna generate `depends_on` task.
 - [ ] Explore task options; consider `retry_on_timeout`: https://docs.databricks.com/aws/en/dev-tools/bundles/job-task-types#other-task-settings
 - [ ]

03.11: STOPPED configuring clusters in `cluster_configs.yml` -> all looks great; also analyzed sample manually created LDP in `example_manual_pipeline.yaml`; continue:
  - [ ] Look into `permissions:` key - search it in bundle-samples



# MY PHILOSOPHY AS A CREATOR / HIGH-LEVEL PLAN FOR THIS REPOSITORY

The idea is right now to build a single comprehensive DABs setup for single use-case, including:
- user, dev, stage, prod environments per UC
- permissions
- all other comprehensive stuff

Then, at some point, we will:
1. Turn this use-case into as reusable template that can be configured and used as a "Custom Template" via databricks bundles CLI.
2. Add new use cases for different environments management. Each use case within separate folder. There will be separate templates for each case. Use case ideas:
2.1. Unity catalog-based separation - all environments reside
in different Unity Catalogs (user/dev/prod/stage) within the same metastore. The most common; the current first use-case.
2.2. Schema-based separation - all environments reside
in different schemas (user/dev/prod/stage) within the same UC.
2.3. Etc.
1. Add plugs layer - the complexity of specific template use-case setup. I'm looking at it as a building blocks, so that for example, some teams don't want to have such complex permissions setup. During the template configuration, when they are prompted in the CLI, we can ask them whether they want a block for overall permissions setup or completely omit it. As a result, I will populate and plug in those blocks into the template.

## TODOs from philosophy

- [ ] Include prerequisites to that Unity Catalogs have to created (specified in `catalog_name` variable); service principals created
- [ ] I don't really like the current folder structure that it contains variables and cluster_configs in separate files and it's all in the root repository folder. I want to move every related Databricks bundles stuff to one folder, maybe have some nested structure there, and have only @databricks.yml file at repository root.

- [ ] separate this repo into different templates for different patterns: env-based catalogs, project-based catalogs
- [ ] When Creating template:
  - [ ] Allow user to select cluster - serverless / custom - prefer to use custom; maybe even just leave serverless commented out or completely remove.



----- BELOW CONTENT GENERATED BY LLM -----

# Databricks Multi-Environment Deployment Template

A production-ready template for deploying Databricks Asset Bundles across multiple environments (user, dev, stage, prod) with environment-specific configurations.

**Focus:** This template demonstrates deployment patterns and environment management, not specific business logic. The sample code is intentionally minimalâ€”replace it with your actual workloads.

## ğŸ¯ What This Template Provides

- **Multi-environment strategy**: Isolated deployments for user, dev, stage, and prod
- **Environment-specific overrides**: Different compute, schedules, and schemas per environment
- **Serverless-first**: Ready for Databricks Free Edition and production
- **Modular structure**: Clean separation between resources and code
- **CI/CD ready**: Git-based deployment workflows

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ databricks.yml              # Bundle configuration with 4 environments
â”œâ”€â”€ variables.yml               # Shared variables
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ sample_ingestion.job.yml         # Example: Ingestion job
â”‚   â”œâ”€â”€ sample_pipeline.pipeline.yml     # Example: DLT pipeline
â”‚   â””â”€â”€ sample_pipeline_trigger.job.yml  # Example: Pipeline trigger
â””â”€â”€ src/
    â”œâ”€â”€ jobs/
    â”‚   â””â”€â”€ ingest_to_raw.py              # Sample job code (replace with yours)
â””â”€â”€ pipelines/
    â”œâ”€â”€ bronze.py           # Sample DLT bronze layer
    â””â”€â”€ silver.py           # Sample DLT silver layer
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Databricks workspace (Free Edition or paid)
- Databricks CLI installed: `pip install databricks-cli`
- Unity Catalog with a catalog (default: `demo-catalog`)

### 2. Deploy to Your Workspace

```bash
# Validate the bundle
databricks bundle validate -t user

# Deploy to your user environment (isolated, development mode)
databricks bundle deploy -t user

# Run the sample job
databricks bundle run sample_ingestion -t user

# Trigger the sample pipeline
databricks bundle run sample_pipeline_trigger -t user
```

### 3. Verify Deployment

Check your Databricks workspace:
- **Jobs**: Look for `[user <yourname>] Sample Ingestion Job`
- **Pipelines**: Look for `[user <yourname>] Sample ETL Pipeline`

## ğŸŒ Environment Strategy

### Four Deployment Targets

| Target | Purpose | Mode | Naming | Schedule | Compute |
|--------|---------|------|--------|----------|---------|
| **user** | Local development | development | `[user <name>]` prefix | Paused | Serverless |
| **dev** | Shared development | production | `[dev]` prefix | Active | Configurable |
| **stage** | Pre-production | production | `[stage]` prefix | Active | Production-like |
| **prod** | Production | production | No prefix | Active | Production |

### Key Differences Per Environment

```yaml
# User: Fast iteration, isolated
user:
  mode: development
  presets:
    name_prefix: "[user ${workspace.current_user.short_name}] "
    trigger_pause_status: PAUSED
    pipelines_development: true

# Dev: Shared team environment
dev:
  mode: production
  git:
    branch: dev
  # Resources optimized for cost

# Stage: Production mirror
stage:
  mode: production
  git:
    branch: stage
  run_as:
    service_principal_name: "<spn-id>"
  # Full production configuration

# Prod: Production
prod:
  mode: production
  git:
    branch: main
  run_as:
    service_principal_name: "<spn-id>"
  # Maximum reliability and performance
```

## ğŸ”’ Permissions and Access Control

This template includes a comprehensive permissions configuration system for managing access to your Databricks resources across environments.

### ğŸš€ Get Started Now

Choose your path:
- **Just testing?** â†’ Deploy immediately with `databricks bundle deploy -t user` (no setup needed!)
- **Team collaboration?** â†’ Follow the [5-minute setup guide](./GET_STARTED_WITH_PERMISSIONS.md)
- **Production ready?** â†’ Full [comprehensive guide](./PERMISSIONS_GUIDE.md)

### ğŸ“– Complete Documentation

| Document | Purpose | Time to Read |
|----------|---------|--------------|
| [**Get Started**](./GET_STARTED_WITH_PERMISSIONS.md) | Choose your path & quick setup | 5 min |
| [**Quick Reference**](./PERMISSIONS_QUICK_REFERENCE.md) | Fast lookup & common patterns | 2 min |
| [**Full Guide**](./PERMISSIONS_GUIDE.md) | Complete setup instructions | 15 min |
| [**Architecture**](./docs/PERMISSIONS_ARCHITECTURE.md) | System design with diagrams | 10 min |
| [**Summary**](./PERMISSIONS_SUMMARY.md) | What was added to template | 5 min |

### ğŸ¯ Quick Facts

- **Three permission levels**: Bundle â†’ Target â†’ Resource (merge together)
- **Four permission types**: `CAN_VIEW`, `CAN_RUN`, `CAN_MANAGE_RUN`, `CAN_MANAGE`
- **Zero config required**: Works immediately in `user` target (development mode)
- **Production ready**: Service principal patterns included
- **Fully documented**: 2000+ lines of examples and guides

## ğŸ”§ Customization

### Replace Sample Code

The `src/` directory contains minimal example code that demonstrates the structure works. **Replace it with your actual code:**

1. **Jobs** (`src/jobs/run.py`): Replace with your ingestion logic
2. **Pipelines** (`src/pipelines/*.py`): Replace with your transformations

### Add Resources

Create new resource files in `resources/`:

```yaml
# resources/my_new_job.job.yml
resources:
  jobs:
    my_new_job:
      name: "My New Job"
      tasks:
        - task_key: main
          spark_python_task:
            python_file: "${workspace.file_path}/src/my_script.py"
```

### Environment-Specific Overrides

Override any resource setting per environment in `databricks.yml`:

```yaml
targets:
  user:
    resources:
      pipelines:
        sample_pipeline:
          schema: "${workspace.current_user.short_name}_schema"
          continuous: false

  prod:
    resources:
      pipelines:
        sample_pipeline:
          schema: "prod_schema"
          continuous: true
          clusters:
            - label: default
              autoscale:
                min_workers: 5
                max_workers: 20
```

## ğŸ“Š Sample Code Overview

The included code is intentionally simple to demonstrate the pattern:

### Ingestion Job (`src/jobs/run.py`)
- Reads from `samples.nyctaxi.trips` (built-in Databricks sample data)
- Prints output to verify the job runs
- **Replace with**: Your actual data ingestion logic

### Bronze Layer (`src/pipelines/bronze.py`)
- Creates a `sample_bronze` table
- Adds ingestion timestamp
- **Replace with**: Your bronze layer transformations

### Silver Layer (`src/pipelines/silver.py`)
- Aggregates bronze data by pickup zone
- **Replace with**: Your silver layer business logic

## ğŸ”„ CI/CD Workflow

### Recommended Git Workflow

```
feature/my-work â†’ dev branch â†’ stage branch â†’ main branch
     â†“               â†“             â†“             â†“
   (local)      dev target    stage target   prod target
```

### Example GitHub Actions

```yaml
name: Deploy to Dev
on:
  push:
    branches: [dev]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy bundle
        run: |
          pip install databricks-cli
          databricks bundle deploy -t dev
        env:
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
```

## ğŸ“‹ Configuration Variables

Edit `variables.yml` for environment-agnostic settings:

```yaml
variables:
  catalog:
    description: UC catalog to deploy into
    default: "`demo-catalog`"

  failure_notification_emails:
    description: Alert recipients
    default: []

  pipeline_min_workers:
    description: Default min workers for pipeline
    default: 1

  photon_enabled:
    description: Enable Photon acceleration
    default: true
```

## ğŸ’¡ Free Edition Notes

The template works on Databricks Free Edition:

- âœ… Uses serverless compute by default
- âœ… Jobs and pipelines supported
- âœ… No custom clusters required
- âš ï¸ Limited to 1 active pipeline per type

To use custom clusters (paid workspaces), uncomment the cluster configurations in `databricks.yml` and resource files.

## ğŸ§ª Testing

```bash
# Validate configuration
databricks bundle validate -t user

# Deploy and test
databricks bundle deploy -t user
databricks bundle run sample_ingestion -t user

# Clean up
databricks bundle destroy -t user
```

## ğŸ” What to Modify for Your Use Case

1. **Bundle name** (`databricks.yml`): Change `realworld_example` to your project name
2. **Sample code** (`src/`): Replace with your actual business logic
3. **Resource files** (`resources/`): Add/modify jobs and pipelines
4. **Variables** (`variables.yml`): Add your configuration variables
5. **Workspace hosts** (`databricks.yml`): Update placeholder URLs
6. **Service principals** (`databricks.yml`): Update for stage/prod
7. **Permissions** (`permissions.yml`, `databricks.yml`): Configure access control for your organization

## ğŸ“š Additional Resources

- [Databricks Asset Bundles Documentation](https://docs.databricks.com/dev-tools/bundles/)
- [Deployment Modes](https://docs.databricks.com/dev-tools/bundles/deployment-modes.html)
- [Delta Live Tables](https://docs.databricks.com/delta-live-tables/)
- [Serverless Compute](https://docs.databricks.com/serverless-compute/)

## ğŸ¤ Contributing

This is a template repository. Fork it, customize it, and make it your own!

## ğŸ“ License

This template is provided as-is for use in your Databricks projects.
